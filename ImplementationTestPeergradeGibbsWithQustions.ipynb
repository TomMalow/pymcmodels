{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETUP LINK TO REDIS CACHE redis://localhost:6379\n",
      "Initializing SAML...\n",
      "Loading local database\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "# Add the application folder to the path\n",
    "import sys\n",
    "sys.path.insert(0,'../Peergrade/peergrade/')\n",
    "\n",
    "import application.model as data_model\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "from bson.objectid import ObjectId\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Grader(object):\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.handins = list()\n",
    "        \n",
    "    def add_handin(self, handin):\n",
    "        self.handins.append(handin)\n",
    "\n",
    "class Question(object):\n",
    "    def __init__(self,id):\n",
    "        self.id = id\n",
    "        self.answers = dict()\n",
    "    \n",
    "    def add_answer(self,grader,value):\n",
    "        self.answers[grader.name] = value\n",
    "\n",
    "class Handin(object):\n",
    "    def __init__(self,title,owner):\n",
    "        self.title = title\n",
    "        self.owner = owner\n",
    "        self.questions = dict()\n",
    "        self.graders = list()\n",
    "    \n",
    "    def add_grader(self,grader):\n",
    "        self.graders.append(grader)\n",
    "        \n",
    "    def add_question(self,question):\n",
    "        self.questions[question.id] = question\n",
    "        \n",
    "class Assignment(object):\n",
    "    \n",
    "    def __init__(self, handins_input, graders_input,n_gradings):\n",
    "        self.graders = dict()\n",
    "        self.handins = dict()\n",
    "        self.n_gradings = n_gradings\n",
    "        for handin in handins_input:\n",
    "            self.handins[handin.title] = handin\n",
    "        for grader in graders_input:\n",
    "            self.graders[grader.name] = grader\n",
    "    \n",
    "    def add_handin(self, handin):\n",
    "        self.handing[handin.title] = handin\n",
    "        \n",
    "    def add_grader(self, grader):\n",
    "        self.graders[grader.title] = grader\n",
    "            \n",
    "class Course(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.assignments = list()\n",
    "        self.handins = dict()\n",
    "        self.graders = dict()\n",
    "        self.n_gradings = 0\n",
    "    \n",
    "    def add_assignment(self,assignment):\n",
    "        self.assignments.append(assignment)\n",
    "        for a in self.assignments:\n",
    "            self.handins.update(a.handins)\n",
    "            self.graders.update(a.graders)\n",
    "        self.n_gradings = self.n_gradings + a.n_gradings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def user_name(user_id):\n",
    "    user = data_model.User.objects.get(id=user_id)\n",
    "    return user.name\n",
    "\n",
    "def question_max_value(question):\n",
    "    if question.question_type == \"boolean\":\n",
    "        return 1\n",
    "    elif question.question_type == \"numerical\":\n",
    "        if question.numericalAnswers:\n",
    "            max_value = max(map(int,question.numericalAnswers.keys()))\n",
    "            return max_value\n",
    "        else:\n",
    "            return 5\n",
    "\n",
    "def answer_value(answer):\n",
    "    if answer.numerical_answer != None:\n",
    "        return answer.numerical_answer / float(question_max_value(answer.question))\n",
    "    if answer.boolean_answer != None:\n",
    "        return answer.boolean_answer / float(question_max_value(answer.question))\n",
    "\n",
    "def answeres_handin(report_grade):\n",
    "    '''Returns a list of tuples containing the answer and the value of the question'''\n",
    "    answers = data_model.Answer.objects(report_grade=report_grade)\n",
    "    result = list()\n",
    "    for answer in answers:\n",
    "        if answer.text_answer == None:\n",
    "            result.append((answer,answer_value(answer)))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_assignment_data(ass_obj):\n",
    "    '''\n",
    "    Takes an course and assignment data model object and transforms it into populated Assignment object\n",
    "    Only student or all?\n",
    "    '''\n",
    "\n",
    "    ## Find all graders\n",
    "    graders = dict()\n",
    "    for student_ in ass_obj.course.students:\n",
    "        graders[str(student_.id)] = Grader(str(student_.id))\n",
    "    \n",
    "    ## Find all handins\n",
    "    handins = dict()\n",
    "    for handin_ in data_model.Handin.objects(assignment=ass_obj):\n",
    "        handins[str(handin_.id)] = Handin(str(handin_.id),str(handin_.submitter.id))\n",
    "        for sec in ass_obj.sections:\n",
    "            for question in sec.questions:\n",
    "                if question.question_type != 'text':\n",
    "                    handins[str(handin_.id)].add_question(Question(question.id))\n",
    "        \n",
    "        \n",
    "    ## Find all handins graders have graded and vice versa\n",
    "    n_gradings = 0\n",
    "    for handin_ in data_model.Handin.objects(assignment=ass_obj):\n",
    "        for grade in data_model.ReportGrade.objects(handin=handin_,state='ANSWERED'):\n",
    "            \n",
    "            n_gradings = n_gradings + 1\n",
    "            \n",
    "            # Needed if TA or Professor have graded reports as they are not initialy part of it\n",
    "            if str(grade.giver.id) not in graders:\n",
    "                graders[str(grade.giver.id)] = Grader(str(grade.giver.id))\n",
    "                \n",
    "            handins[str(handin_.id)].add_grader(graders[str(grade.giver.id)])\n",
    "            \n",
    "            ## Find all answers to each question\n",
    "            for answer, answer_value in answeres_handin(grade):\n",
    "                handins[str(handin_.id)].questions[answer.question.id].add_answer(grade.giver,answer_value)\n",
    "            \n",
    "    ## update reference in graders\n",
    "    for handin in handins.itervalues():\n",
    "        for grader in handin.graders:\n",
    "            grader.add_handin(handin)\n",
    "\n",
    "    return Assignment(handins.itervalues(),graders.itervalues(),n_gradings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def fetch_data(obj):\n",
    "\n",
    "    res_c = Course()\n",
    "        \n",
    "    if type(obj).__name__ == \"Course\":\n",
    "        assignments_d = data_model.Assignment.objects(course=obj)\n",
    "        for assignment_d in assignments_d:\n",
    "            res_c.add_assignment(fetch_assignment_data(assignment_d))\n",
    "    elif type(obj).__name__ == \"Assignment\":\n",
    "        res_c.add_assignment(fetch_assignment_data(obj))\n",
    "        \n",
    "    return res_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c = data_model.Course.objects.get(title=\"Computational Tools for Big Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "ObjectId('56543675325f23000fb0a637')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5f7c521562b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-af02ccec2647>\u001b[0m in \u001b[0;36mfetch_data\u001b[0;34m(obj)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0massignments_d\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAssignment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcourse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0massignment_d\u001b[0m \u001b[0;32min\u001b[0m \u001b[0massignments_d\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mres_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_assignment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massignment_d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Assignment\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mres_c\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_assignment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetch_assignment_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-19b5273c5fd5>\u001b[0m in \u001b[0;36mfetch_assignment_data\u001b[0;34m(ass_obj)\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0;31m## Find all answers to each question\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer_value\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mansweres_handin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrade\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mhandins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandin_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrade\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgiver\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0manswer_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m## update reference in graders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: ObjectId('56543675325f23000fb0a637')"
     ]
    }
   ],
   "source": [
    "t = fetch_data(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gibbs_model(data):\n",
    "    \n",
    "    # Counts\n",
    "    N_H = len(data.handins) # Number of handins\n",
    "    N_G = len(data.graders) # Number of graders\n",
    "    N_g = data.n_gradings   # Number of gradings\n",
    "    N_eval = N_g*N_G   # Number of evaluations in total\n",
    "    \n",
    "    # Hyperparameters\n",
    "    ga_h = 0.5\n",
    "    la_h = 1.0\n",
    "    al_h = 10.0\n",
    "    be_h = 0.1\n",
    "\n",
    "    ga_g = 0.0\n",
    "    la_g = 1.0\n",
    "    al_g = 50.0\n",
    "    be_g = 0.1\n",
    "    \n",
    "    al_e = 10.0\n",
    "    be_e = 1.0\n",
    "    t_h = 500.0\n",
    "    t_g = 100.0\n",
    "    \n",
    "    # Prior parameters\n",
    "    u_h = dict()\n",
    "    t_h = dict()\n",
    "    u_g = dict()\n",
    "    t_g = dict()\n",
    "    T = dict()\n",
    "    B = dict()\n",
    "\n",
    "    # Draw from priors\n",
    "    e = np.random.gamma(al_e,1.0/be_e)\n",
    "    for handin in data.handins.itervalues():\n",
    "        for q in handin.question.iterkeys():\n",
    "            t_h[q] = np.random.gamma(al_h,1.0/be_h)\n",
    "            u_h[q] = np.random.normal(ga_h,np.sqrt(1.0/(la_g * t_h[h])))\n",
    "            T[q] = np.random.normal(u_h[a],np.sqrt(1.0/t_h[a]))\n",
    "    for grader in data.graders.itervalues():\n",
    "        for handin in grader.handins\n",
    "            for q in handin.question.iterkeys():\n",
    "            for \n",
    "            t_g[g] = np.random.gamma(al_g,1.0/be_g)\n",
    "            u_g[g] = np.random.normal(ga_g,np.sqrt(1.0/(la_g * t_g[g])))\n",
    "            B[g] = np.random.normal(u_g[g],np.sqrt(1.0/t_g[g]))\n",
    "\n",
    "    # Gibbs sampling\n",
    "    \n",
    "    burn_in = 1000  # warm-up steps\n",
    "    samples = 5000 # Gibbs sampling steps\n",
    "    \n",
    "    # Tracers initialising\n",
    "    acc_e = 0\n",
    "    acc_u_h = defaultdict(lambda : 0)\n",
    "    acc_t_h = defaultdict(lambda : 0)\n",
    "    acc_u_g = defaultdict(lambda : 0)\n",
    "    acc_t_g = defaultdict(lambda : 0)\n",
    "    acc_T = defaultdict(lambda : 0)\n",
    "    acc_B = defaultdict(lambda : 0)\n",
    "\n",
    "    for r in range(burn_in + samples):\n",
    "        print \"\\r%i\" % (r+1) + \" out of %i\" % (burn_in + samples),\n",
    "        # Sample T\n",
    "        for handin in data.handins.itervalues():\n",
    "            for answer+values in handin.answers.iteritems():\n",
    "                n_gradings = len(handin.graders)\n",
    "                sum_ = 0.0\n",
    "                sum_ = sum_ + answer.value - B[g]\n",
    "                v = e*n_gradings+t_h[h]\n",
    "                T[a] = np.random.normal((u_h[h]*t_h[h]+e*sum_)/v,np.sqrt(1/v))\n",
    "            \n",
    "        # Sample B\n",
    "        for g, grader in data.graders.iteritems():\n",
    "            n_gradings = len(grader.handins)\n",
    "            sum_ = 0.0\n",
    "            for h in grader.handins:\n",
    "                for answers in handin.answers[g].itervalues():\n",
    "                    for answer in answers\n",
    "                        sum_ = sum_ + answer.value - T[h.title]\n",
    "            v = e*n_gradings+t_g[g]\n",
    "            B[g] = np.random.normal((u_g[g]*t_g[g]+e*sum_)/v,np.sqrt(1/v))\n",
    "        \n",
    "        # Sample e\n",
    "        sum_ = 0.0\n",
    "        for h, handin in data.handins.iteritems():\n",
    "            for g, answers in handin.answers.iteritems():\n",
    "                for answer in answers\n",
    "                    sum_ = sum_ + np.square(answer - (T[h]+B[g]))\n",
    "        e = np.random.gamma(al_e+0.5*N_eval,1.0/(be_e+0.5*sum_))\n",
    "\n",
    "        # Sample u_h and t_h\n",
    "        for h in data.handins.iterkeys():\n",
    "            la_ = (la_h+1.0)\n",
    "            al_ = al_h + 0.5 * la_h + 0.5 * np.square(T[h]-u_h[h])\n",
    "            be_ = be_h + 0.5 + 0.5 * 1.0\n",
    "#            al_ = al_h+0.5\n",
    "#            be_ = be_h+0.5*((la_h*np.square(T[h]-ga_h))/la_)\n",
    "            t_h[h] = np.random.gamma(al_,1.0/be_)\n",
    "            u_h[h] = np.random.normal((la_h*ga_h+T[h])/la_,np.sqrt(1.0/(la_*t_h[h])))\n",
    "\n",
    "        # Sample u_g and t_g\n",
    "        for g in data.graders.iterkeys():\n",
    "            la_ = (la_g+1)\n",
    "            al_ = al_g + 0.5 * la_g + 0.5 * np.square(B[g]-u_g[g])\n",
    "            be_ = be_g + 0.5 + 0.5 * 1\n",
    "#            al_ = al_g+0.5\n",
    "#            be_ = be_g+0.5*((la_g*np.square(B[g]-ga_g))/la_)\n",
    "            t_g[g] = np.random.gamma(al_,1.0/be_)\n",
    "            u_g[g] = np.random.normal((la_g*ga_g+B[g])/la_,np.sqrt(1.0/(t_g[g])))\n",
    "            \n",
    "        # Collect tracings\n",
    "        if r > burn_in:\n",
    "            acc_e = acc_e + e\n",
    "            for h in data.handins.iterkeys():\n",
    "                acc_u_h[h] = acc_u_h[h] + u_h[h]\n",
    "                acc_t_h[h] = acc_t_h[h] + t_h[h]\n",
    "                acc_T[h] = acc_T[h] + T[h]\n",
    "            for g in data.graders.iterkeys():\n",
    "                acc_u_g[g] = acc_u_g[g] + u_g[g]\n",
    "                acc_t_g[g] = acc_t_g[g] + t_g[g]\n",
    "                acc_B[g] = acc_B[g] + B[g]    \n",
    "    \n",
    "    acc_e = acc_e / float(samples)\n",
    "    for h in data.handins.iterkeys():\n",
    "        acc_u_h[h] = acc_u_h[h] / float(samples)\n",
    "        acc_t_h[h] = acc_t_h[h] / float(samples)\n",
    "        acc_T[h] = acc_T[h] / float(samples)\n",
    "    for g in data.graders.iterkeys():\n",
    "        acc_u_g[g] = acc_u_g[g] / float(samples)\n",
    "        acc_t_g[g] = acc_t_g[g] / float(samples)\n",
    "        acc_B[g] = acc_B[g] / float(samples)\n",
    "    \n",
    "    traces = {'e' : acc_e,\n",
    "              'u_h' : acc_u_h,\n",
    "              't_h' : acc_t_h,\n",
    "              'u_g' : acc_u_g,\n",
    "              't_g' : acc_t_g,\n",
    "              'T' : acc_T,\n",
    "              'B' : acc_B}\n",
    "\n",
    "    return traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_bias(t,result):\n",
    "    scores = list()\n",
    "    for name, g in t.graders.iteritems():\n",
    "        scores.append((user_name(ObjectId(name)),result['B'][name]))\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    scores.sort(key=lambda x:x[1])\n",
    "    model_1 = map(lambda x : float(x[1]),scores)\n",
    "\n",
    "    label = map(lambda x : x[0],scores)\n",
    "    y = xrange(0,len(scores))\n",
    "    plt.plot(model_1,y,'or',label=\"Gibbs sampling\",alpha=0.65)\n",
    "\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0,\n",
    "                     box.width+0.45, box.height*5])\n",
    "    plt.yticks(xrange(0,len(scores)),label)\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlim(-0.3,0.3)\n",
    "    plt.vlines(0, 0, len(scores), color=\"k\", linestyles=\"--\", lw=1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def plot_handin_score(t,result):\n",
    "\n",
    "    scores = list()\n",
    "    for title, handin in t.handins.iteritems():\n",
    "        scores.append((title,result['T'][title],handin.gradeings.values()))\n",
    "\n",
    "    ax = plt.subplot(111)\n",
    "\n",
    "    scores.sort(key=lambda x:x[1])\n",
    "    model_1 = map(lambda x : float(x[1]),scores)\n",
    "    mean = map(lambda x : x[2],scores)\n",
    "\n",
    "    label = map(lambda x : x[0],scores)\n",
    "    y = xrange(0,len(scores))\n",
    "    plt.plot(model_1,y,'or',label=\"Gibbs sampling\")\n",
    "    plt.boxplot(mean, positions=y, vert=False)\n",
    "#    plt.plot(mean,y,'ob',label=\"Mean\",alpha=0.65)\n",
    "\n",
    "    box = ax.get_position()\n",
    "    ax.set_position([box.x0, box.y0,\n",
    "                     box.width+0.45, box.height*7])\n",
    "    plt.yticks(xrange(0,len(scores)),label)\n",
    "    plt.legend(loc=4)\n",
    "    plt.xlim(0,1)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_sorted_result(t,result):\n",
    "    s = list()\n",
    "    for name, g in t.graders.iteritems():\n",
    "        s.append((user_name(ObjectId(name)),result['B'][name]))\n",
    "    s.sort(key=lambda x:x[1])\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The entire course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c = data_model.Course.objects.get(title=\"Computational Tools for Big Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "course_data = fetch_data(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "course_result = gibbs_model(course_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0 / course_result['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias(course_data,course_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_handin_score(course_data,course_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = data_model.Assignment.objects.get(title=\"UNIX, Python and Fast Data\")\n",
    "\n",
    "a1_data = fetch_data(a1)\n",
    "\n",
    "a1_result = gibbs_model(a1_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0 / a1_result['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias(a1_data,a1_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_handin_score(a1_data,a1_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a2 = data_model.Assignment.objects.get(title=\"Databases and Streaming\")\n",
    "\n",
    "a2_data = fetch_data(a2)\n",
    "\n",
    "a2_result = gibbs_model(a2_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0 / a2_result['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias(a2_data,a2_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_handin_score(a2_data,a2_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a3 = data_model.Assignment.objects.get(title=\"Big Data and Deep Learning\")\n",
    "\n",
    "a3_data = fetch_data(a3)\n",
    "\n",
    "a3_result = gibbs_model(a3_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0 / a3_result['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias(a3_data,a3_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a3_data,a3_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_handin_score(a3_data,a3_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a4 = data_model.Assignment.objects.get(title=\"Your Choice of Subject\")\n",
    "\n",
    "a4_data = fetch_data(a4)\n",
    "\n",
    "a4_result = gibbs_model(a4_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "1.0 / a4_result['e']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_bias(a4_data,a4_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_handin_score(a4_data,a4_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## closer look at  Jesper Bitcsh Østergaard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(course_data,course_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Jesper Bitsch \\xd8stergaard':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a1_data,a1_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Jesper Bitsch \\xd8stergaard':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a2_data,a2_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Jesper Bitsch \\xd8stergaard':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a3_data,a3_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Jesper Bitsch \\xd8stergaard':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a4_data,a4_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Jesper Bitsch \\xd8stergaard':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of the bias over the 4 assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((-0.18)+(-0.019)+(-0.11)+(-0.05))/4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the bias in assignment 4 have a very heavy influence on the bias of the entire course which is not ideal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer look at André Castro Lundin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(course_data,course_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Andr\\xe9 Castro Lundin':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a1_data,a1_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Andr\\xe9 Castro Lundin':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a2_data,a2_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Andr\\xe9 Castro Lundin':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a3_data,a3_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Andr\\xe9 Castro Lundin':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a4_data,a4_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if name == u'Andr\\xe9 Castro Lundin':\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of the bias over the 4 assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((-0.0189)+(-0.035)+(-0.02)+(0.19))/4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same as previously but this time it is in the positive direction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closer look at myself, Thomas Kjærgaard Malowanczyk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(course_data,course_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if u'Thomas Kjærgaard Malowanczyk' in name:\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a1_data,a1_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if u'Thomas Kjærgaard Malowanczyk' in name:\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a2_data,a2_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if u'Thomas Kjærgaard Malowanczyk' in name:\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a3_data,a3_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if u'Thomas Kjærgaard Malowanczyk' in name:\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sr = get_sorted_result(a4_data,a4_result)\n",
    "\n",
    "for name, value in sr:\n",
    "    if u'Thomas Kjærgaard Malowanczyk' in name:\n",
    "        print name, value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average of the bias over the 4 assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "((-0.0926)+(-0.000356)+(-0.139)+(-0.0799))/4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see a more normal bias where I have received a negtive bias in most assignments. But the bias from the entore course is still lower than any of the bias in the assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this we can conclude that using MCMC on the entire course does not provide better result even though it provide more data.\n",
    "The reson for the extreme result is hard to define as it is an unsubervised method so the underlying reasons in MCMC is not definable.\n",
    "It can be assumed that taking MCMC over multiple assignments introduces more variables as each assignment can differe widely in both the subject, quality of the questions and the knowledge of the graders in both the subject and their abillity to grade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
